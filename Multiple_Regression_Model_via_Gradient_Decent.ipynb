{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression: Multiple Regression (gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the code below is to generate a simple regression model (that takes a single input to make a prediction) and compare its performance to a multiple regression model (that takes more than one input to make a prediction.) The models are built in the context of the input, gene expression of a gene familiy, and how it affects the output, effector gene expression. The goal is test this code with existing gene expression repositories such as the NIH Gene Expression Omni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gene_data = graphlab.SFrame('gene_expression_panel.gl/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # note this allows us to refer to numpy as np instead "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# below is a user defined function used to quickly convert data sets into a \"numpy\" matrix\n",
    "def get_numpy_data(data_sframe, features, output):\n",
    "    data_sframe['constant'] = 1 #add a column for a constant to generate a y-intercept\n",
    "    features = ['constant'] + features \n",
    "    features_sframe = data_sframe[features] #extract the features/genes of interest in addition to the column with the constants\n",
    "    # the following line will convert the features_SFrame into a numpy matrix:\n",
    "    feature_matrix = features_sframe.to_numpy()\n",
    "    output_sarray = data_sframe[output]\n",
    "    output_array = output_sarray.to_numpy()\n",
    "    return(feature_matrix, output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#input the column headers that contain the feature of interest such as 'gene_family' and the output of interest 'effector_gene_expression'\n",
    "#from the collection of data named 'gene_data'\n",
    "(example_features, example_output) = get_numpy_data(gene_data, ['gene_family1'], 'effector_gene_expression') # the [] around 'sqft_living' makes it a list\n",
    "#convert the data type into a matrix for downstream calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting output given regression weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_output(feature_matrix, weights):\n",
    "    #once we have the initial weights of each feature--gene expression of a gene family--we can predict the expression levels of the \n",
    "    #effector gene by the for an individual sample.\n",
    "    #KEY: this is for an individual sample and we will ultimately iterate through many rows(samples) of a feature/gene expression\n",
    "    #matrix. Afterwards we will optimize the model by finding the version of the model with the\n",
    "    #smallest residual sum of squares value.\n",
    "    predictions = np.dot(feature_matrix, weights)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Derivative of the Squared Difference of Predicted Values\n",
    "# and Actual Outputs to Calculate the Weights of Features/inputs of Gene Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we need to find the derivative of each feature over all data points and the errors(difference between predictions and\n",
    "#actual outputs according to data. This is necessary to determine the weights of each feature that will be in the final\n",
    "#model\n",
    "def feature_derivative(errors, feature):\n",
    "    dot_prod = np.dot(errors, feature)\n",
    "    derivative = 2*dot_prod #simplified and cleaned up derivative of the squared difference of predicted outputs and actual\n",
    "    #outputs of the data\n",
    "    return(derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent (used to find the regression model with the minimized residual sum squared error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance):\n",
    "    converged = False \n",
    "    weights = np.array(initial_weights)\n",
    "    while not converged:\n",
    "        predictions = predict_output(feature_matrix, weights)\n",
    "        errors = predictions - output\n",
    "        gradient_sum_squares = 0\n",
    "        # while we haven't reached the tolerance yet, update each feature's weight\n",
    "        for i in range(len(weights)): # loop over each weight\n",
    "            derivative = feature_derivative(errors, feature_matrix[:, i])\n",
    "            gradient_sum_squares = derivative*derivative + gradient_sum_squares\n",
    "            weights[i] = weights[i] - (derivative*step_size)\n",
    "        gradient_magnitude = sqrt(gradient_sum_squares)\n",
    "        if gradient_magnitude < tolerance:\n",
    "            converged = True\n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Gradient Descent as Simple Regression and Putting all of the User defined Functions Above Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data,test_data = sales.random_split(.8,seed=0)\n",
    "#split the data into a training set to find the weights of features and to find the optimal model from this subset of\n",
    "#data.\n",
    "\n",
    "#the test data is used to predict the expected output given a feature/gene expression level the model has not \"seen\" before\n",
    "#afterwards the error of this model will be calculated according to the actual outputs that the model has not 'seen' before\n",
    "#as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "simple_features = ['gene_family1']\n",
    "my_output = 'effector_gene'\n",
    "\n",
    "\n",
    "(simple_feature_matrix, output) = get_numpy_data(train_data, simple_features, my_output) ###split the training data\n",
    "#into the features/input gene expression and the actual output/effector gene expression. Remember, the actual output is\n",
    "#used to check how accurate the model predictions are.\n",
    "\n",
    "initial_weights = np.array([-47000., 1.])\n",
    "step_size = 7e-12\n",
    "tolerance = 2.5e7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1_weights = regression_gradient_descent(simple_feature_matrix, output, initial_weights, step_size, tolerance)\n",
    "#the regression_gradient_function gives us the weights of the features that will be used to make the predicted\n",
    "#outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the test data is split into the feature inputs and outputs just like the training data. The goal is to feed the model\n",
    "#inputs/gene expression levels it has not \"seen\" before and then we use the test data outputs that correspond to the inputs\n",
    "#to see how well the model performs\n",
    "\n",
    "(test_simple_feature_matrix, test_output) = get_numpy_data(test_data, simple_features, my_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1_predictions = predict_output(test_simple_feature_matrix, model1_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#to see how well the model did, the residual sum squared (RSS) needs to be calculated\n",
    "#afterwards another model is generated but with more input features to be trained on\n",
    "#then on this second model we will predict how well the second model performs on test data again\n",
    "#lastly, we will compare the RSS of these two models to see which is the best of the two.\n",
    "model1_residual_square = (model1_predictions - test_output)**2\n",
    "model1_RSS = model1_residual_square.sum()\n",
    "print model1_RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a multiple regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_features = ['gene_family1', 'gene_family2'] # this time we will use two gene families with the intention that the\n",
    "#model will perform better with more information\n",
    "\n",
    "my_output = 'effector gene'\n",
    "(feature_matrix, output) = get_numpy_data(train_data, model_features, my_output)\n",
    "(test_feature_matrix, test_output) = get_numpy_data(test_data, model_features, my_output)\n",
    "initial_weights = np.array([-100000., 1., 1.])\n",
    "step_size = 4e-12\n",
    "tolerance = 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2_weights = regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "model2_predictions = predict_output(test_feature_matrix, model2_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2_residual_square = (model2_predictions - test_output)**2\n",
    "model2_RSS = model2_residual_square.sum()\n",
    "print model2_RSS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
